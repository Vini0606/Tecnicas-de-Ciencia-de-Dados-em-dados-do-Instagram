{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e338c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projetos\\Tecnicas de NLP em dados do Instagram\\av_nlp_insta\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adiciona o diret√≥rio raiz do projeto ao sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from config import settings\n",
    "import google.generativeai as genai\n",
    "from typing import Mapping, List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.representation import BaseRepresentation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import emoji\n",
    "import spacy\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # Baixar se for a primeira vez\n",
    "stop_words_pt = stopwords.words('portuguese')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "API_KEY = os.getenv(\"API_GEMINI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffea214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.read_excel(settings.ALL_XLSX, sheet_name=\"reels_latestComments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb3143",
   "metadata": {},
   "source": [
    "# An√°lise de Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b5ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Inicializa o pipeline (seu c√≥digo original est√° perfeito)\n",
    "# O modelo 'cardiffnlp/twitter-xlm-roberta-base-sentiment' retorna os r√≥tulos: 'Positive', 'Negative', 'Neutral'\n",
    "analisador_sentimento = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    ")\n",
    "\n",
    "# 2. Modifica a fun√ß√£o para retornar tanto o LABEL quanto o SCORE\n",
    "def analisar_sentimento_completo(texto):\n",
    "    \"\"\"\n",
    "    Esta fun√ß√£o recebe um texto, analisa o sentimento e retorna um\n",
    "    pd.Series com o r√≥tulo e a pontua√ß√£o, pronto para ser adicionado\n",
    "    a um DataFrame.\n",
    "    \"\"\"\n",
    "    if pd.notna(texto) and texto.strip() != \"\":\n",
    "        try:\n",
    "            # O resultado √© uma lista com um dicion√°rio, ex: [{'label': 'Positive', 'score': 0.99}]\n",
    "            resultado = analisador_sentimento(texto, truncation=True, max_length=512)[0]\n",
    "            label = resultado['label']\n",
    "            score = resultado['score']\n",
    "            return pd.Series([label, score])\n",
    "        except Exception as e:\n",
    "            # Retorna None em caso de erro na an√°lise\n",
    "            return pd.Series([None, None])\n",
    "    else: \n",
    "        # Retorna None se o texto for nulo ou vazio\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# 3. Aplica a nova fun√ß√£o e cria DUAS novas colunas de uma vez\n",
    "# O 'apply' vai expandir o pd.Series retornado para as colunas especificadas\n",
    "df_comments[['sentiment_label', 'sentiment_score']] = df_comments['text'].apply(analisar_sentimento_completo)\n",
    "\n",
    "# 4. Verifique o resultado\n",
    "print(\"DataFrame com as novas colunas:\")\n",
    "print(df_comments[['text', 'sentiment_label', 'sentiment_score']].head())\n",
    "\n",
    "print(\"\\nDistribui√ß√£o dos sentimentos:\")\n",
    "df_comments['sentiment_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d678eb0",
   "metadata": {},
   "source": [
    "# Pr√©-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a507ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover stop words de um texto\n",
    "def remover_stopwords(texto):\n",
    "    if not isinstance(texto, str):  # Se n√£o for string, retorna vazio\n",
    "        return \"\"\n",
    "    palavras = texto.split()\n",
    "    palavras_filtradas = [p for p in palavras if p.lower() not in stop_words_pt]\n",
    "    return \" \".join(palavras_filtradas)\n",
    "\n",
    "# Aplicar a todos os coment√°rios negativos\n",
    "df_comments['text clean'] = df_comments['text'].apply(remover_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd71653",
   "metadata": {},
   "source": [
    "# Modelagem de T√≥picos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3975aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiDocsRefiner(BaseRepresentation):\n",
    "    def __init__(self, api_key: str, model: str = \"gemini-2.0-flash\", prompt_template: str = None):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = model\n",
    "        # Prompt aprimorado para receber exemplos de documentos\n",
    "        self.prompt_template = prompt_template or (\n",
    "            \"Escreva uma descri√ß√£o de um par√°grafo que descreva detalhadamente o que os coment√°rios do instagram presentes neste t√≥pico tem em comum: {documents}\"\n",
    "        )\n",
    "\n",
    "    def extract_topics(\n",
    "        self,\n",
    "        topic_model,\n",
    "        documents: pd.DataFrame, # Recebe o DataFrame de documentos\n",
    "        c_tf_idf: csr_matrix,\n",
    "        topics: Mapping[str, List[Tuple[str, float]]]\n",
    "    ) -> Mapping[str, List[Tuple[str, float]]]:\n",
    "        \n",
    "        updated_topics = {}\n",
    "        # Mapeia cada documento ao seu t√≥pico para f√°cil acesso\n",
    "        docs_per_topic = documents.groupby(['Topic'])['Document'].apply(list)\n",
    "\n",
    "        for topic_id, keywords in topics.items():\n",
    "            \n",
    "            if topic_id == -1:\n",
    "                updated_topics[topic_id] = keywords\n",
    "                continue\n",
    "\n",
    "            if topic_id % 10 == 0:\n",
    "                time.sleep(60)\n",
    "            \n",
    "            try:\n",
    "                # Pega os documentos reais do t√≥pico\n",
    "                topic_docs = docs_per_topic.get(topic_id, [])\n",
    "                if not topic_docs:\n",
    "                    updated_topics[topic_id] = keywords\n",
    "                    continue\n",
    "\n",
    "                # Pega uma amostra para n√£o exceder o limite do prompt\n",
    "                sample_size = min(len(topic_docs), 10) # Amostra de at√© 10 documentos\n",
    "                docs_sample = \"\\n- \".join(random.sample(topic_docs, sample_size))\n",
    "                \n",
    "                prompt = self.prompt_template.format(documents=docs_sample)\n",
    "\n",
    "                response = genai.GenerativeModel(self.model).generate_content(prompt)\n",
    "                label = response.text.strip() if hasattr(response, \"text\") else None\n",
    "\n",
    "                if label:\n",
    "                    # Adiciona o novo r√≥tulo com a maior pontua√ß√£o\n",
    "                    updated_keywords = [(label, 1.0)] + keywords \n",
    "                    updated_topics[topic_id] = updated_keywords\n",
    "                else:\n",
    "                    updated_topics[topic_id] = keywords  # Fallback\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[GeminiDocsRefiner] Erro no t√≥pico {topic_id}: {e}\")\n",
    "                updated_topics[topic_id] = keywords  # Fallback\n",
    "\n",
    "        return updated_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b492261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dataframes = []\n",
    "\n",
    "representation_model = GeminiDocsRefiner(api_key=API_KEY)\n",
    "\n",
    "# Regex: captura palavras com 2+ caracteres OU qualquer caractere no bloco de Emojis Unicode\n",
    "# Isso mant√©m as palavras normais e adiciona os emojis como tokens\n",
    "emoji_pattern = r\"(?u)\\b\\w\\w+\\b|[\\U0001F300-\\U0001F5FF\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF\\u2600-\\u26FF\\u2700-\\u27BF]\"\n",
    "\n",
    "vectorizer_model = CountVectorizer(token_pattern=emoji_pattern)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    embedding_model=\"rufimelo/bert-large-portuguese-cased-sts\",\n",
    "    language=\"multilingual\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Supondo que df_comments √© seu DataFrame\n",
    "# A fun√ß√£o demojize transforma 'üòÇ' em ':face_with_tears_of_joy:'\n",
    "# Usar language='pt' pode ajudar a obter descri√ß√µes em portugu√™s, se dispon√≠veis.\n",
    "def demojize_text(text):\n",
    "    return emoji.demojize(text, language='pt')\n",
    "\n",
    "df_comments['text_demojized'] = df_comments['text clean'].apply(demojize_text)\n",
    "\n",
    "# Agora, use esta nova coluna para a modelagem\n",
    "docs_completos = list(df_comments['text_demojized'])\n",
    "\n",
    "print(f\"Iniciando a modelagem de t√≥picos em {len(docs_completos)} documentos...\")\n",
    "topics, probs = topic_model.fit_transform(docs_completos)\n",
    "print(\"Modelagem global de t√≥picos conclu√≠da.\")\n",
    "\n",
    "# Obt√©m as informa√ß√µes de t√≥pico para TODOS os documentos\n",
    "topic_info_df = topic_model.get_document_info(docs_completos)\n",
    "\n",
    "# Reseta o √≠ndice de ambos para garantir o alinhamento\n",
    "df_comments_reset = df_comments.reset_index(drop=True)\n",
    "topic_info_df_reset = topic_info_df.reset_index(drop=True)\n",
    "\n",
    "# Concatena os DataFrames originais com os resultados da modelagem\n",
    "df_final = pd.concat([df_comments_reset, topic_info_df_reset], axis=1)\n",
    "\n",
    "# (Opcional) Remove a coluna de documento duplicada\n",
    "df_final = df_final.drop(columns=['Document'])\n",
    "\n",
    "print(\"DataFrame final com resultados globais:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63befb",
   "metadata": {},
   "source": [
    "# Salvar dados Modelados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ad852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 2. Use o pd.ExcelWriter em um bloco 'with' para garantir que o arquivo seja salvo e fechado corretamente\n",
    "    with pd.ExcelWriter(\n",
    "        settings.ALL_XLSX,\n",
    "        mode=\"a\",  # 'a' significa \"append\" (adicionar/anexar)\n",
    "        engine=\"openpyxl\",\n",
    "        if_sheet_exists=\"replace\"  # Substitui a aba se ela j√° existir\n",
    "    ) as writer:\n",
    "        \n",
    "        # 3. Salve seu DataFrame na aba especificada\n",
    "        df_final.to_excel(writer, sheet_name='reels_latestComments', index=False)\n",
    "        # 'index=False' √© importante para n√£o salvar o √≠ndice do DataFrame como uma coluna no Excel\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Aviso: O arquivo '{settings.ALL_XLSX}' n√£o foi encontrado.\")\n",
    "    print(\"Um novo arquivo ser√° criado com a aba especificada.\")\n",
    "    # Se o arquivo n√£o existir, o modo 'a' falha. Ent√£o, simplesmente o criamos.\n",
    "    df_final.to_excel('reels_latestComments.xlsx', sheet_name='reels_latestComments', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "av_nlp_insta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
